{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from yaml import safe_load\n",
    "import json\n",
    "random.seed(42)\n",
    "from opendet2.data.voc_coco import VOC_COCO_CATEGORIES\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "file_stem = '/media/prakash/LaCie1/Backup/opendet2/8gpu_outputs/faster_rcnn_R_50_FPN_3x_opendet_run1_aw_0.017_lw_0.21_best_model/pascal_voc_eval/'\n",
    "logits = []\n",
    "labels = []\n",
    "######\n",
    "### Read Logits from the assiciated data\n",
    "### This means the logits of only the detections are displayed.\n",
    "#######\n",
    "# Read the JSON file\n",
    "tmp_arr = []\n",
    "tmp_label = []\n",
    "tmp_name = []\n",
    "concerned_ids =  np.arange(20).tolist() + [80]\n",
    "TOTAL_CLASSES=VOC_COCO_CATEGORIES\n",
    "for id in range (len(TOTAL_CLASSES)):\n",
    "    with open(file_stem + TOTAL_CLASSES[id]+'embeddings.json' , 'r') as file:\n",
    "        data = json.load(file)\n",
    "    # Collate all the array lists into one NumPy array\n",
    "    for cnt,key in enumerate(data):\n",
    "        if id==80:\n",
    "            max_det = 1000\n",
    "        else:\n",
    "            max_det=300\n",
    "        if (id in concerned_ids and cnt<max_det ): #and int(key)<1050):\n",
    "            tmp_arr.append(np.array(data[key]))\n",
    "            #tmp_label.append(id)     \n",
    "            tmp_label.append(id) \n",
    "\n",
    "total_feat = np.array(tmp_arr)#[1000:-1,:]\n",
    "labels = np.array(tmp_label)#[100:-1]\n",
    "print (total_feat.shape)\n",
    "print (np.unique(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "#### UMAP\n",
    "#######\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import random\n",
    "from matplotlib.colors import ListedColormap\n",
    "random.seed(42)\n",
    "plt.figure()\n",
    "clusterable_embedding = umap.UMAP(\n",
    "    n_neighbors=400,\n",
    "    min_dist=1,\n",
    "    n_components=3,\n",
    "    random_state=40,\n",
    ").fit_transform(total_feat )\n",
    "\n",
    "#clusterable_embedding = umap.UMAP(n_components=3,min_dist=1,n_neighbors=300,random_state=42).fit_transform(total_feat )\n",
    "generic_fontsize = 16\n",
    "\n",
    "'''rev_names = {}\n",
    "for k,v in classes.items():\n",
    "    v=v-1\n",
    "    rev_names[k] = v '''\n",
    "\n",
    "label_colors = {0: '#FF0000', 1: '#00FF00', 2: '#0000FF', 3: '#FFFF00', 4: '#FF00FF',\n",
    "                5: '#00FFFF', 6: '#FFA500', 7: '#800080', 8: '#FFA500' , 9: '#008080',\n",
    "                10: '#800000', 11: '#808000', 12: '#1f77b4', 13: '#00FF80', 14: '#008000' ,\n",
    "                15: '#ff7f0e', 16: '#808080', 17: '#80FFFF', 18: '#0080FF', 19: '#8000FF',\n",
    "                20: '#FF8000', 21: '#008080', 22: '#80FF00', 23: '#FF8080', 24: '#80FFFF',\n",
    "                80: '#000000'}\n",
    "\n",
    "\n",
    "# Create a scatter plot\n",
    "for label in set(labels):\n",
    "    #if label  not in [80]:\n",
    "        #print (label_colors[label])\n",
    "        mask = [l == label for l in labels]\n",
    "        scatter = plt.scatter(np.array(clusterable_embedding[:, 0])[mask], np.array(clusterable_embedding[:, 1])[mask], label=label, color=label_colors[label])\n",
    "        plt.legend(*scatter.legend_elements(),fontsize = 15)\n",
    "#scatter = plt.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], c=np.array(labels) , cmap=COLORMAP)\n",
    "#plt.title(rev_names)\n",
    "\n",
    "plt.xticks( fontsize=generic_fontsize, weight='bold')\n",
    "plt.yticks(fontsize=generic_fontsize, weight='bold')\n",
    "#plt.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1],def open_url(hoverData):\n",
    "if clusterable_embedding.shape[1]==3:\n",
    "    fig = plt.figure(figsize=(17, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for label in set(labels):\n",
    "        #if label  not  in [80]:\n",
    "            mask = [l == label for l in labels]\n",
    "            if label == 80:\n",
    "                mark = 'x'\n",
    "            else:\n",
    "                mark = 'o'\n",
    "            scatter = ax.scatter(np.array(clusterable_embedding[:, 0])[mask], np.array(clusterable_embedding[:, 1])[mask], np.array(clusterable_embedding[:, 2])[mask],\n",
    "                    label=label, c=label_colors[label], marker=mark)\n",
    "\n",
    "    ax.legend(*scatter.legend_elements(),fontsize = 15)\n",
    "    #plt.title(rev_names)\n",
    "\n",
    "    #plt.xticks( fontsize=generic_fontsize, weight='bold')\n",
    "    #plt.yticks(fontsize=generic_fontsize, weight='bold')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12, width=3, length=6, pad=6)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=10, width=2, length=4, pad=6)\n",
    "    # Set z-axis label\n",
    "    #ax.set_zlabel('Third Dimension',fontsize=generic_fontsize)\n",
    "    ax.view_init(elev=13,azim=0)\n",
    "    ax.legend()\n",
    "    plt.title('Logit Space Visualisation of Detections on Holdout test set',fontsize = generic_fontsize)\n",
    "    # Set legend font properties\n",
    "    #legend = plt.legend(fontsize=30)\n",
    "    # Get the legend handles and labels\n",
    "    #handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    # Increase font size of x-axis go\n",
    "    #plt.xlabel('First Dimension', fontsize=generic_fontsize)\n",
    "    #plt.ylabel('Second Dimension', fontsize=generic_fontsize)\n",
    "    #plt.zlabel('Third Dimension', fontsize=generic_fontsize)\n",
    "    plt.savefig('3d.pdf',dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data: Replace this with your actual data\n",
    "embeddings = total_feat\n",
    "\n",
    "# Calculate the means of each class\n",
    "class_means = {}\n",
    "centroid = []\n",
    "for label in np.unique(labels):\n",
    "    #print (embeddings[labels == label].shape)\n",
    "    centroid.append(np.mean(embeddings[labels == label], axis=0))\n",
    "    class_means[label] =  np.mean(embeddings[labels == label], axis=0)\n",
    "#print (class_means )\n",
    "# Calculate the intraclass variance\n",
    "\n",
    "intraclass_variance = 0.0\n",
    "\n",
    "intra_dist_mean = []\n",
    "intra_dist_var = []\n",
    "for label in np.arange(20):\n",
    "    \n",
    "    #tmp_det = np.array(embeddings[labels == label]).shape [0]\n",
    "    #print (class_means[label])\n",
    "    #print (embeddings[labels == label])\n",
    "    #print (np.linalg.norm(embeddings[labels == label] - class_means[label])**2)\n",
    "    #print (label)\n",
    "\n",
    "    #det +=tmp_det\n",
    "    intra_dist_mean.append(np.mean(np.linalg.norm(embeddings[labels == label] - class_means[label], axis=1) ))\n",
    "    intra_dist_var.append(np.var(np.linalg.norm(embeddings[labels == label] - class_means[label], axis=1)  ))\n",
    "    #intraclass_variance +=  np.linalg.norm(embeddings[labels == label] - class_means[label])**2\n",
    "#print (intra_dist_mean)\n",
    "# Calculate the mean distance\n",
    "intra_mean_distance = np.mean(intra_dist_mean)\n",
    "\n",
    "# Calculate the variance of distances\n",
    "intra_variance = np.mean(intra_dist_var)\n",
    "\n",
    "#print(\"Mean Distance:\", intra_mean_distance)\n",
    "print ('mean of intra distance', intra_mean_distance)\n",
    "print(\"Variance of Distances:\", intra_variance)\n",
    "\n",
    "# Calculate pairwise distances between class means\n",
    "\n",
    "# Calculate pairwise distances between class means\n",
    "distances = []\n",
    "for i in range(len(class_means) - 1):\n",
    "    for j in range(i + 1, len(class_means)):\n",
    "        dist = np.linalg.norm(class_means[i] - class_means[j])  # Euclidean distance\n",
    "        distances.append(dist)\n",
    "\n",
    "# Calculate the mean distance\n",
    "mean_distance = np.mean(distances)\n",
    "\n",
    "# Calculate the variance of distances\n",
    "#variance = np.var(distances)\n",
    "\n",
    "print(\"Mean of inter distances :\", np.mean(distances))\n",
    "print(\"variance of inter Distances:\", np.var(distances))\n",
    "print (distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def my_dunn_index_optimized(cluster_centers, cluster_labels, data):\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    n_clusters = len(unique_labels)\n",
    "    \n",
    "    # Calculate diameters of clusters\n",
    "    diameters = []\n",
    "    for k in range(n_clusters):\n",
    "        cluster_points = data[cluster_labels == k]\n",
    "        if len(cluster_points) > 1:\n",
    "            diameters.append(np.max(cdist(cluster_points, cluster_points)))\n",
    "        else:\n",
    "            diameters.append(0.0)\n",
    "    \n",
    "    max_diameter = max(diameters)\n",
    "\n",
    "    # Calculate min distance between points in different clusters\n",
    "    min_distances = []\n",
    "    for i in range(n_clusters):\n",
    "        for j in range(i + 1, n_clusters):\n",
    "            points_i = data[cluster_labels == i]\n",
    "            points_j = data[cluster_labels == j]\n",
    "            if len(points_i) > 0 and len(points_j) > 0:\n",
    "                min_distances.append(np.min(cdist(points_i, points_j)))\n",
    "    \n",
    "    min_distance = min(min_distances) if min_distances else 0.0\n",
    "    print (min_distance)\n",
    "    print (max_diameter)\n",
    "    dunn_index = min_distance / max_diameter if max_diameter > 0 else 0.0\n",
    "\n",
    "    return dunn_index\n",
    "\n",
    "my_dunn_index_optimized(class_means, labels,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "import numpy as np\n",
    "\n",
    "calinski_harabasz_index = calinski_harabasz_score(embeddings, labels)\n",
    "\n",
    "print(\"Calinski-Harabasz Index:\", calinski_harabasz_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def hubert_index(data, labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    m = len(unique_labels)\n",
    "\n",
    "    # Calculate the mean distance within clusters\n",
    "    mean_within_cluster_distance = sum(\n",
    "        np.mean(cdist(data[labels == label], data[labels == label]))\n",
    "        for label in unique_labels if np.sum(labels == label) > 1\n",
    "    ) / m\n",
    "\n",
    "    # Calculate the mean distance between clusters\n",
    "    mean_between_cluster_distance = sum(\n",
    "        np.mean(cdist(data[labels == i], data[labels == j]))\n",
    "        for i in range(m)\n",
    "        for j in range(i + 1, m)\n",
    "    ) / (m * (m - 1) / 2)\n",
    "\n",
    "    # Calculate the Hubert Index\n",
    "    hubert_index_value = mean_within_cluster_distance / mean_between_cluster_distance\n",
    "\n",
    "    return hubert_index_value\n",
    "\n",
    " \n",
    "\n",
    "hubert_index_value = hubert_index(embeddings, labels)\n",
    "print(\"Hubert Index:\", hubert_index_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "# Perform Fisher's LDA\n",
    "lda = LinearDiscriminantAnalysis(n_components=3)\n",
    "X_lda = lda.fit_transform(embeddings, labels)\n",
    "\n",
    "# Analyze separation between clusters and intra-cluster properties\n",
    "print(\"Fisher's LDA Separation Metric:\", lda.score(embeddings, labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "def xie_beni_index(data,labels,centroids):\n",
    "    # Fit KMeans clustering model\n",
    "    k = len(np.unique(labels) )\n",
    "    kmeans = KMeans(n_clusters= k)\n",
    "\n",
    "\n",
    "    # Calculate the within-cluster dispersion\n",
    "    dist_within_cluster = np.sum([np.sum((data[labels == i] - centroids[i])**2) for i in range(k)])\n",
    "\n",
    "    # Calculate the distance from each point to the nearest cluster center\n",
    "    dist_to_nearest_center = pairwise_distances_argmin_min(data, centroids)[1]\n",
    "\n",
    "    # Calculate the Xie-Beni Index\n",
    "    xie_beni_index_value = dist_within_cluster / (k * np.sum(dist_to_nearest_center))\n",
    "\n",
    "    return xie_beni_index_value\n",
    "\n",
    "xie_beni_index_value = xie_beni_index(embeddings, labels,centroid)\n",
    "print(\"Xie-Beni Index:\", xie_beni_index_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.stats import entropy\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "def calculate_free_energy(embeddings, kde_in, kde_out):\n",
    "    log_dens_in = kde_in.score_samples(embeddings)\n",
    "    log_dens_out = kde_out.score_samples(embeddings)\n",
    "\n",
    "    free_energy_scores = -log_dens_in + log_dens_out\n",
    "    return free_energy_scores\n",
    "\n",
    "# Convert NumPy array to PyTorch tensor\n",
    "embeddings_tr = torch.from_numpy(embeddings)\n",
    "def plot_pdf(ax, data, label, color):\n",
    "    sns.histplot(data, kde=True, ax=ax, label=label, color=color)\n",
    "    ax.set_title('Free Energy PDF')\n",
    "    ax.set_xlabel('Free Energy')\n",
    "    ax.legend()\n",
    "# Create KDE models for in-distribution and out-of-distribution embeddings\n",
    "in_distribution_embeddings = total_feat[(labels >= 0)  & (labels <20)]  #np.random.multivariate_normal(mean=[0, 0], cov=[[1, 0.5], [0.5, 1]], size=1000)\n",
    "out_distribution_embeddings =  total_feat[labels == 80]   #np.random.multivariate_normal(mean=[3, 3], cov=[[1, -0.5], [-0.5, 1]], size=1000)\n",
    "\n",
    "\n",
    "kde_in = KernelDensity(bandwidth=0.5, kernel='gaussian')\n",
    "kde_in.fit(in_distribution_embeddings)\n",
    "\n",
    "kde_out = KernelDensity(bandwidth=0.5, kernel='gaussian')\n",
    "kde_out.fit(out_distribution_embeddings)\n",
    "\n",
    "# Calculate Free Energy scores for both in-distribution and out-of-distribution embeddings\n",
    "free_energy_in = calculate_free_energy(in_distribution_embeddings, kde_in, kde_out)\n",
    "free_energy_out = calculate_free_energy(out_distribution_embeddings, kde_in, kde_out)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plot_pdf(ax, free_energy_in, 'In-Distribution', 'blue')\n",
    "plot_pdf(ax, free_energy_out, 'Out-of-Distribution', 'orange')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Using the tSNE for visualization\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 9))\n",
    "# Apply t-SNE to reduce the dimensionality of the feature space\n",
    "tsne = TSNE(n_components=3, random_state=1)  #TSNE(n_components=3,  perplexity=10, learning_rate=20)\n",
    "#TSNE(n_components=3, random_state=0) \n",
    "embedded = tsne.fit_transform(total_feat)\n",
    "import pandas as pd\n",
    "tsne_df = pd.DataFrame({'X':embedded[:,0],\n",
    "                        'Y':embedded[:,1]})\n",
    "tsne_df['label'] = labels\n",
    "# Visualize the t-SNE embedding\n",
    "#sns.scatterplot(x=\"X\", y=\"Y\",\n",
    "#              data=tsne_df, palette=COLORMAP)\n",
    "#\n",
    "# Create a scatter plot\n",
    "for label in set(labels):\n",
    "    #if label  not in [0,1,2,3,4]:\n",
    "        #print (label_colors[label])\n",
    "        mask = [l == label for l in labels]\n",
    "        scatter = plt.scatter(np.array(embedded[:, 0])[mask], np.array(embedded[:, 1])[mask], label=label, color=label_colors[label])\n",
    "        plt.legend(*scatter.legend_elements(),fontsize = 15)\n",
    "#scatter = plt.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], c=np.array(labels) , cmap=COLORMAP)\n",
    "#plt.title(rev_names)\n",
    "plt.xticks( fontsize=generic_fontsize, weight='bold')\n",
    "plt.yticks(fontsize=generic_fontsize, weight='bold')\n",
    "#plt.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1],def open_url(hoverData):\n",
    "\n",
    "if embedded.shape[1]==3:\n",
    "    fig = plt.figure(figsize=(17, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for label in set(labels):\n",
    "        mask = [l == label for l in labels]\n",
    "        scatter = ax.scatter(np.array(embedded[:, 0])[mask], np.array(embedded[:, 1])[mask], np.array(embedded[:, ])[mask],\n",
    "                    label=label, c=label_colors[label], marker='o')\n",
    "\n",
    "    ax.legend(*scatter.legend_elements(),fontsize = 15)\n",
    "    #plt.title(rev_names)\n",
    "    #ax.scatter(tsne_df['X'], tsne_df['Y'],embedded[:,2], c=np.array(labels) , cmap=COLORMAP)\n",
    "    ax.legend(*scatter.legend_elements())\n",
    "    \n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Dimension 3')\n",
    "    #ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opendet_drone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
